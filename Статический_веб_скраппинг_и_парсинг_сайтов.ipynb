{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Статический веб-скраппинг и парсинг сайтов",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpVWArAu0PH5kzPQMaQb5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timofeyegorov/education/blob/main/%D0%A1%D1%82%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B2%D0%B5%D0%B1_%D1%81%D0%BA%D1%80%D0%B0%D0%BF%D0%BF%D0%B8%D0%BD%D0%B3_%D0%B8_%D0%BF%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3_%D1%81%D0%B0%D0%B9%D1%82%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JsiUjkEpRu"
      },
      "source": [
        "# Немного теории"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "nv-sDqn4WR7E",
        "outputId": "8e590a34-a8ae-4176-e96a-eb811f07d411"
      },
      "source": [
        "from IPython.display import Image # Библиотека для отображения картинок\n",
        "\n",
        "display(Image(url='https://prowebscraper.com/blog/wp-content/uploads/2017/11/Web_Scraping_for_Non-Programmers.png', \n",
        "              width = 1400) # Задаем ширину картинки\n",
        "       ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://prowebscraper.com/blog/wp-content/uploads/2017/11/Web_Scraping_for_Non-Programmers.png\" width=\"1400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81TtNXgDXPkV"
      },
      "source": [
        "**Веб-скраппинг** - получения веб-данных путем извлечения их со страниц веб-ресурсов. Некоторые сайты предоставляют специальный API для программного доступа к своим данным.\n",
        "\n",
        "\n",
        "**Парсинг** - анализ и обработка полученных данных.\n",
        "\n",
        "Минусы веб-скраппинга.\n",
        "- Он хрупкий (веб-страницы, которые вы очищаете, могут часто меняться).\n",
        "- Это может быть запрещено (некоторые веб-приложения имеют политику против скрапинга).\n",
        "- Это может быть медленным и экспансивным (если вам нужно забирать и пропускать много шума).\n",
        "__________________________________________________\n",
        "Вебскраппинг делится на статический и динамический.\n",
        "\n",
        "**Статический** - получает статический код страницы. Игнорирует JavaScript. Можно выполнить с помощью библиотек requests и BeautifulSoup. С помощью requests отправляют запрос на получение информации с веб-страницы, а BeautifulSoup нужен, чтобы распарсить полученную html-разметку.\n",
        "\n",
        "**Динамический** - использует фактический браузер и позволяет JavaScript работать. Иногда вам необходимо автоматизировать браузер, моделируя пользователя, чтобы получить необходимый контент. Используется библиотека Selenium. Она имтирует работе браузера, то можно нажимать на кнопки, заполнять формы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59nN4ELNxxP4"
      },
      "source": [
        "# Скраппинг базы автомобилей с Авто.ру с BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7DcJ1NHDZWb"
      },
      "source": [
        "**Для чего нужна задержка для скраппинга.**\n",
        "\n",
        "Если вы будете парсить более-менее серьезные сайты - можете столкнутся с защитой от парсинга.\n",
        "Например, если парсить какой-то сайт достаточно длительное время и слать запросы к нему очень часто - этот сайт может вас заблокировать по ip и выдать капчу. Кроме того, это несет нагрузку на сайт, и хорошо бы поставить задержку, чтобы сильно не нагружть сервер.\n",
        "Основной принцип обхода защиты от парсинга - ваш парсер должен вести себя так, как вел бы себя человек, зашедший на сайт. Для каждого сайта задержка подбирается индивидуально."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ROl0frJblz"
      },
      "source": [
        "from bs4 import BeautifulSoup  # импорт класса BeautifulSoup из модуля bs4 Для парсинга страниц\n",
        "import requests                # Для отправки запросов к сайту\n",
        "from requests import get       # Запрос на получение данных\n",
        "from tqdm import tqdm\n",
        "import urllib.request          # библиотка для скачивания изображений\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZkh9NxuF2Ko"
      },
      "source": [
        "# Ссылка на страницу, которую парсим, куда динамически подставляем номер страницы\n",
        "url = f'https://auto.ru/novosibirsk/cars/all/?page=1'\n",
        "response = get(url)         # Получаем статус ответа сервера, 200 - все ок\n",
        "\n",
        "response.encoding                                             # Смотрим какая кодировка используется\n",
        "content = response.content                                    # Получаем байтовое представление данных для нетекстовых запросов\n",
        "html_soup = BeautifulSoup(content.decode('utf-8','ignore'))   # Декодируем данные, игнорируя ошибки в символах"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrF4rl2rSXku"
      },
      "source": [
        "start_time = time.time()        # Засекаем время выполнения кода в ячейке\n",
        "cars = []                       # Список для хранения спарсенных данных для каждого объявления\n",
        "count = 1                       # Счетчик страниц сайта, начинаем извлекать данные с первой страницы\n",
        "\n",
        "while count <= 5:               # Устанавливаем максимальное количество страниц с которых будем собирать данные\n",
        "    print(f'Страница {count}')  # Информация для вывода о том, какая страница сейчас парсится\n",
        "\n",
        "    # Ссылка на страницу, которую парсим, куда динамически подставляем номер страницы\n",
        "    url = f'https://auto.ru/novosibirsk/cars/all/?page={count}'\n",
        "    response = get(url)         # Получаем статус ответа сервера, 200 - все ок\n",
        "    \n",
        "    response.encoding                                             # Смотрим какая кодировка используется\n",
        "    content = response.content                                    # Получаем байтовое представление данных для нетекстовых запросов\n",
        "    html_soup = BeautifulSoup(content.decode('utf-8','ignore'))   # Декодируем данные, игнорируя ошибки в символах\n",
        "\n",
        "    #html_soup = BeautifulSoup(response.text, 'html.parser') - заменяет предыдущие 3 строки кода, если нет проблем с кодировкой\n",
        "\n",
        "\n",
        "    cars_data = html_soup.find_all('div', class_ = 'ListingItem-module__description') # Находим все div блоки с классом 'ListingItem-module__main'\n",
        "                                                                               # Метод возвращает список, где каждый div блок - это отдельное объявление\n",
        "\n",
        "    if cars_data != []:                                                        # Проверяем, что данные есть\n",
        "      cars.extend(cars_data)                                                   # Добавляем в список данных данные всех объявлений с текущей страницы\n",
        "      print(f'Со страницы спарсено {len(cars_data)} объявлений, всего объявлений спарсено: {len(cars)}')\n",
        "\n",
        "      # Генерируем задержку до следующего запроса от 1 до 11 секунд\n",
        "      value = random.random()                                               # Генерируем случайное число от 0 до 1\n",
        "      time_sleep = 1 + value*10                                             # Добавляем случайное число к единице\n",
        "      print(f'Задержка до следующего запроса: {round(time_sleep, 1)} сек')\n",
        "      time.sleep(time_sleep)                                                # Откладываем исполнение кода на time_sleep секунд\n",
        "      print('--------------------------------')\n",
        "\n",
        "    else:\n",
        "      print('empty')                                                        # Если данные не справились - прекращаем цикл\n",
        "      break  \n",
        "    count += 1                                                              # Увеличиваем счетчик на 1 (переходим к парсингу следующей страницы)\n",
        "\n",
        "end_time = time.time() - start_time                                         # Считаем сколько времени ушло на парсинг\n",
        "print()\n",
        "print(f'Время извлечения данных: {round(end_time/60, 1)} минут')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzecR6NKiAnC"
      },
      "source": [
        "count = 0       # Обнуляем счетчик, будем извлекать нужную информацию с нулевого элемента\n",
        "data = []       # Список для хранения фильтрованной информации по каждому объявлению\n",
        "skip_ads = 0    # Счетчик пропущенных объявления\n",
        "\n",
        "# Задаем, какое кол-во объявлений спарсим\n",
        "while count <= (len(cars)-1):\n",
        "  try:                                                                            # Применяем try - except, чтобы обработать ошибку, когда не указана цена авто\n",
        "    info = cars[int(count)]                                                       # Получаем блок div по объявлению\n",
        "    title = info.find('a', {'class':'Link ListingItemTitle-module__link'}).text   # Находим тег 'a' с классом 'ListingItem-module__main', извлекаем текстовую информацию\n",
        "    price = info.find('div', {'class':'ListingItemPrice-module__content'}).text   # Находим тег div с классом 'ListingItem-module__main', извлекаем текстовую информацию\n",
        "\n",
        "    other_info = info.find_all('div', {'class':'ListingItemTechSummaryDesktop__cell'}) # Находим ВСЕ теги div с классом 'ListingItemTechSummaryDesktop__cell' (их 5 штук), метод возвращает список\n",
        "\n",
        "    # Для каждого из пяти тегов div с классом 'ListingItemTechSummaryDesktop__cell' получаем текстовую информацию\n",
        "    characteristics = str(other_info[0].get_text())\n",
        "    transmission = str(other_info[1].get_text())\n",
        "    body = str(other_info[2].get_text())\n",
        "    drive = str(other_info[3].get_text())\n",
        "    color = str(other_info[4].get_text())\n",
        "\n",
        "    # Выводим данные которые спарсили для каждого объявления\n",
        "    print(count, '. ', title, ' - ', price, ' - ', characteristics, ' - ', transmission, ' - ', body, ' - ', drive, ' - ', color, sep = '')\n",
        "\n",
        "    temp_list = [title, price, characteristics, transmission, body, drive, color] # Во временный список добавляем все спарсенные данные по текущему объявлению\n",
        "    data.append(temp_list)                                                        # Добавляем данные по текущему объявлению в дата для дальнейшего преобразования в пандас ДатаФрейм\n",
        "  except AttributeError:\n",
        "    skip_ads +=1                                                                # Считаем кол-во пропущенных объявлений\n",
        "    print(count)\n",
        "  count += 1                                                                    # Увеличиваем счетчик для перехода к парсингу следующего объявления\n",
        "print()\n",
        "print(f'Пропущено {skip_ads} объявлений')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5bkY7Pn_yul"
      },
      "source": [
        "columns = ['title', 'price', 'characteristics', 'transmission', 'body', 'drive', 'color']\n",
        "df = pd.DataFrame(data, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK15YhYd_zo9"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hECdPmFaj4S"
      },
      "source": [
        "# Скраппинг изображений с BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oROTU-ikK0Ps"
      },
      "source": [
        "link_root = f'http://www.motorpage.ru'                                          # Корневая ссылка на сайт, откуда будем парсить объявления\n",
        "response = get(link_root + '/media/photos.html')                                # Получение информации о галерее изображений\n",
        "html_soup = BeautifulSoup(response.text)                                        # Получаем разметку html-страницы для последующей обработки\n",
        "\n",
        "cars_data = html_soup.find_all('a', {'class': 'col-xs-12 col-sm-6 zero-padding'})\n",
        "cars_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzKUWpVdK5Yy"
      },
      "source": [
        "cars_data[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4WBXv0BLGiv"
      },
      "source": [
        "brand_links = [link['href'] for link in cars_data]   \n",
        "brand_links[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI9qwaSwLNB9"
      },
      "source": [
        "brand_names = [link[1:link.rfind('/')].capitalize() for link in brand_links] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLhgSdtULWZE"
      },
      "source": [
        "('acura').capitalize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jfpggmSL0ni"
      },
      "source": [
        "brand_dict = {k:v for k, v in zip(brand_names, brand_links)}                    # Создаем словарь, где ключ - название бренда, значение - окончание ссылки на страницу с фото\n",
        "number_cars = html_soup.find_all('figcaption', {'class': 'caption-photos'})     # По каждому авто парсим информацию по количеству фото\n",
        "cars_values = [int(cars.text) for cars in number_cars]        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stNSvkEgL2XG"
      },
      "source": [
        "brand_numbers = list(zip(brand_names, cars_values)) \n",
        "brand_numbers[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6nBaBflaj4a"
      },
      "source": [
        "# Соберем информацию в текстовом виде о наличии фотографий для каждой марки авто\n",
        "\n",
        "link_root = f'http://www.motorpage.ru'                                          # Корневая ссылка на сайт, откуда будем парсить объявления\n",
        "response = get(link_root + '/media/photos.html')                                # Получение информации о галерее изображений\n",
        "html_soup = BeautifulSoup(response.text)                                        # Получаем разметку html-страницы для последующей обработки\n",
        "\n",
        "cars_data = html_soup.find_all('a', {'class': 'col-xs-12 col-sm-6 zero-padding'})  # Находим блоки с информацией о количестве фото по каждой марке авто\n",
        "\n",
        "brand_links = [link['href'] for link in cars_data]                              # Получение окончания ссылки для каждого бренда, например, Acura/photos.html (полный адрес: http://www.motorpage.ru/Acura/photos.html)\n",
        "brand_names = [link[1:link.rfind('/')].capitalize() for link in brand_links]    # Получение имени каждого бренда из окончания ссылки\n",
        "\n",
        "brand_dict = {k:v for k, v in zip(brand_names, brand_links)}                    # Создаем словарь, где ключ - название бренда, значение - окончание ссылки на страницу с фото\n",
        "number_cars = html_soup.find_all('figcaption', {'class': 'caption-photos'})     # По каждому авто парсим информацию по количеству фото\n",
        "cars_values = [int(cars.text) for cars in number_cars]                          # Получаем тексотовую информацию о количестве фото по каждому авто из number_cars\n",
        "\n",
        "brand_numbers = list(zip(brand_names, cars_values))                             # Создаем список кортежей из парных элементов, где первый элемент - бренд авто, второй - количество фотографий этого бренда\n",
        "brand_numbers.sort(key = lambda x: x[1], reverse = True)                        # Делаем сортировку списка по количеству фото (второму элементу кортежа)\n",
        "\n",
        "# Выводим информацию по количеству фотографий для каждого бренда\n",
        "num_ = 1\n",
        "for i, j in (brand_numbers):\n",
        "  print(f'{num_}. Для бренда {i} можно скачать {j} изображений')\n",
        "  num_ += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEa5SxnNabuM"
      },
      "source": [
        "# Выбор брендов для скачивания изображений\n",
        "selected_brands = [brand_name for brand_name, brand_num in brand_numbers[79:82]]\n",
        "selected_brands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWYTRnK92JlL"
      },
      "source": [
        "['country',\t'Сколько_вам_лет',\t'В_какой_сфере_сейчас_работаете',\t'Ваш_средний_доход_в_месяц',\t'Рассматриваете_ли_в_перспективе_платное_обучение_профессии_Разработчик_Искусственного_Интеллекта',\t'Сколько_времени_готовы_выделить_на_обучение_в_неделю']:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ1G-u_a53ah"
      },
      "source": [
        "country, age, job, earnings, training, time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z58DfwXaXYGa"
      },
      "source": [
        "data_path = './cars_images/'          # Указание пути для сохранения данных\n",
        "os.makedirs(data_path, exist_ok=True) # Создание папки для данных, если её не существует"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs-B1YMPY2mm"
      },
      "source": [
        "# Сначала спарсим все ссылки на фотографии\n",
        "start_time = time.time()                                                        # Засекаем время выполнения кода в ячейке\n",
        "count = 1                                                                       # Счетчик страниц сайта, начинаем извлекать данные с первой страницы\n",
        "link_dict = {}                                                                  # Словарь для хранения ссылок на фото для брендов {бренд: ссылки}\n",
        "\n",
        "for brand_name in selected_brands:                                              # Итерация по рассматриваемым брендам\n",
        "  print(f'{count}. Получение ссылок для бренда: {brand_name}')                  # Информация о процессе\n",
        "  \n",
        "\n",
        "  current_link = link_root + brand_dict[brand_name]                             #  Получаем текущую ссылку (пример: http://www.motorpage.ru/Hummer/photos.html)\n",
        "  \n",
        "  response = requests.get(current_link)                                         # Делаем запрос на сайт и получаем ответ\n",
        "  html_soup = BeautifulSoup(response.text)                                      # Получаем html для обработки информации\n",
        "\n",
        "  model_cards = html_soup.findAll('a', {'class': \"col-xs-12 col-sm-4 col-md-3\"}) # По тегу и классу получаем блоки с информацией по каждой модели выбранной марки\n",
        "  model_links = [model_card['href'] for model_card in model_cards]              # Получение ссылок на изображения по каждой марке каждого бренда\n",
        "  brand_links = []                                                              # Хранилище для ссылок на изображения машин данного бренда\n",
        "\n",
        "  for model_link in tqdm(model_links):                                          # итерация по всем ссылкам на изображения с моделями\n",
        "      direct_link = link_root + model_link                                      # Получаем ссылку, пример: http://www.motorpage.ru/Hummer/H2/last/photos/\n",
        "      response = requests.get(direct_link)                                      # Делаем запрос на сайт\n",
        "      html_soup = BeautifulSoup(response.text)                                  # Получаем объект для обработки информации\n",
        "      picture_frames = html_soup.findAll('img', {'title': True})                # Собираем все блоки с тегом img, {'title': True} поможет найти только блоки с изображениями, где у которых есть заголовок\n",
        "      picture_links = [frame['src'] for frame in picture_frames]                # Сбор ссылок на исходные изображенения \n",
        "      brand_links += picture_links                                              # Добавляем в список brand_links все ссылки на изображения по текущей модели авто   \n",
        "  link_dict[brand_name] = brand_links                                           # Добавляем в словарь link_dict ключ бренда авто и список ссылок на изображения в качестве значения\n",
        "\n",
        "\n",
        "  # Генерируем задержку до следующего запроса от 1 до 4 секунд\n",
        "  value = random.random()                                                       # Генерируем случайное число от 0 до 1\n",
        "  time_sleep = 1 + value*3                                                      # Добавляем случайное число к единице\n",
        "  print(f'Задержка до следующего запроса: {round(time_sleep, 1)} сек')\n",
        "  time.sleep(time_sleep)                                                        # Откладываем исполнение кода на time_sleep секунд\n",
        "  count +=1\n",
        "  print('--------------------------------')\n",
        "\n",
        "end_time = time.time() - start_time                                             # Считаем сколько времени ушло на парсинг\n",
        "print()\n",
        "print(f'Время извлечения данных: {round(end_time/60, 1)} минут')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Topr8lovDFBz"
      },
      "source": [
        "# Скачиваем изображения\n",
        "\n",
        "start_time = time.time()                                                        # Засекаем время выполнения кода в ячейке\n",
        "count = 1                                                                       # Счетчик страниц сайта, начинаем извлекать данные с первой страницы\n",
        "\n",
        "for brand_name, link_list in link_dict.items():                                 # В цикле берем бренд и список ссылок на его изображение\n",
        "  print(f'{count}. Скачивание изображений для бренда: {brand_name}')            # Выводим инфо процесса\n",
        "  os.makedirs(data_path + brand_name, exist_ok=True)                            # Создание папки бренда, если её не существует\n",
        "  dir_path = data_path + brand_name                                             # Указываем текущую директорию, куда сохраняем изображения\n",
        "\n",
        "  for idx, link in enumerate(tqdm(link_list)):                                  # Проходим по каждой ссылке\n",
        "    urllib.request.urlretrieve(link, f\"{dir_path}/{'0'*(4-len(str(idx)))}{idx}_{brand_name}.jpg\") # Сохраняем изображение\n",
        "\n",
        "  count +=1\n",
        "  print()\n",
        "  print('--------------------------------')\n",
        "\n",
        "end_time = time.time() - start_time                                             # Считаем сколько времени ушло на парсинг\n",
        "print()\n",
        "print(f'Время скачивания: {round(end_time/60, 1)} минут')      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1hd03URihVQ"
      },
      "source": [
        "# Получение данных с kinopoisk.ru через API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhTJp1VebrXs"
      },
      "source": [
        "**Что такое API**\n",
        "\n",
        "API (Application programming interface) — это контракт, который предоставляет программа. «Ко мне можно обращаться так и так, я обязуюсь делать то и это».\n",
        "\n",
        "Подробнее: https://habr.com/ru/post/464261/\n",
        "\n",
        "https://kinopoisk.cloud/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMHc_-PE7PQ"
      },
      "source": [
        "# Получаем и изучаем json файл\n",
        "\n",
        "api_token='Ваш Токен'   # Токен, доступный после регистрации\n",
        "\n",
        "url = f'https://api.kinopoisk.cloud/movies/{1143242}/token/Ваш Токен'    # Url для запроса данных с сайта\n",
        "request = get(url)              # Отправляем запрос, получаем ответ\n",
        "data = request.json()           # Преобразуем ответ в json\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HsgftawYgpv"
      },
      "source": [
        "# Пример отсутствия фильма по id\n",
        "\n",
        "url = f'https://api.kinopoisk.cloud/movies/{1143244}/token/c9972b77e92bbc443c28279e1a29ec58'\n",
        "request = get(url) # Отправляем запрос, получаем ответ\n",
        "data = request.json() \n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo3qvrnANSMX"
      },
      "source": [
        "start_time = time.time() # Засекаем время выполнения кода в ячейке\n",
        "\n",
        "films_data = [] # Список для хранения данных по всем фильмам\n",
        "\n",
        "# Названия ключей json файла, к кторым будем обращаться для получения нужных данных\n",
        "params = ['title', 'id_kinopoisk', 'rating_imdb', 'rating_kinopoisk', 'year', 'actors', 'budget', 'composers', 'directors', 'fees_world', 'genres', 'description']\n",
        "skip_films = 0 # Счетчик пропущенных id\n",
        "count = 1 # Счетчик итераций\n",
        "\n",
        "for movie_number in range(1143242, 1143242+90):  # Перебираем id фильмов\n",
        "    try:\n",
        "        url = f'https://api.kinopoisk.cloud/movies/{movie_number}/token/c9972b77e92bbc443c28279e1a29ec58' # Передаем id фильма в url\n",
        "        request = get(url)  # Отправляем запрос, получаем ответ\n",
        "        data = request.json()  # Получаем json файл\n",
        "\n",
        "        print(f\"{count}. Получение информации для фильма '{data['title']}'\") # Информация о процессе\n",
        "        temp_list = []                                    # Создаем временный список, куда вносим данные по текущему фильму\n",
        "        for param in params:                              # Проходимся по нужным нам ключам в json файле\n",
        "           temp_list.append(data[param])                  # Добавляем данные во временный список\n",
        "        temp_list.append(data['collapse']['duration'])    # Отдельно добавляем информацию по длительности фильма\n",
        "        films_data.append(temp_list)                      # В список films_data добалвяем как элемент временный список temp_list\n",
        "        count += 1\n",
        "\n",
        "    except KeyError:                                      # В случае, если фильма нет, выводим об этом сообщение\n",
        "        print(f'{count} фильм с id {movie_number} отсутствует в базе')\n",
        "        count += 1\n",
        "        skip_films += 1 \n",
        "\n",
        "end_time = time.time() - start_time                                         # Считаем сколько времени ушло на парсинг\n",
        "print()\n",
        "print(f'Время извлечения данных: {round(end_time/60, 1)} минут')\n",
        "print(f'Пропущено {skip_films} id')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKVQbNZTTDgu"
      },
      "source": [
        "df = pd.DataFrame(films_data, columns=['title', 'id', 'Рейтинг IMDB', 'Рейтинг Кинопоиск', 'Год', 'Актёры', 'Бюджет', 'Композиторы', 'Режиссёр', 'Сборы', 'Жанры', 'Описание', 'Длительность']) # создаем датафрейм"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd1ofI6XURJQ"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}